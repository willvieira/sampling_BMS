# Running the GRTS in R

## Setup

The following packages are used in the process to run the GRTS.
For the `spsurvey` R package, we are using the version `v5.3.0`, and this code only works in versions superior to `v5.0`.
To ensure consistent use of the code, we suggest loading the same environment used in this project, which is stored in the renv.lock file.
To load the same environment, follow the steps below:

```r
# install `renv` package if necessary
if (!require(renv)) install.packages('renv')
# Restore the project environment
renv::activate(project = "/path/to/project")
```

This function will install all necessary packages with their correct version. Make sure to replace /path/to/project with the path to the directory where the project is stored. This will ensure that all the required packages and their specific versions are installed and loaded correctly.

```{r loadPackages,results='hide',message=FALSE,warning=FALSE}
library(raster)
library(exactextractr)
library(tidyverse)
library(spatstat)
library(sf)
library(spsurvey)
library(MBHdesign)

# to reproduce the same results
set.seed(0.0)
```

## parameters

```{r parameters}
# Maximum proportion of NA pixels (non-habitat) in a hexagon
# if 0.8, it means an hexagon has to have at least 20% of habitat pixels
prop_na = 0.8

# Percentage of hexagons to cover a region
sample_effort = 0.02

# Total sample size for SSU (Main + Over)
# It must be an even numbers
ssu_N = 6

# Number of replications when running the GRTS
nb_rep = 10

# Code of ecoregions to sample
eco_sim = c(
    '7', '28', '30', '31',
    '46', '47', '48', '49',
    '73', '77', '78', '86',
    '72', '74', '75', '76',
    '96', '99', '100', '101',
    '102', '103', '117',
    '216', '217'
)

# Name of file and columns to extract legacy
legacyFile = file.path('..', 'data', 'legacySites.csv')
lat = 'latitude'
lon = 'longitude'

# Buffer size (in Km) to adjust inclusion probability of hexagons around legacy sites
bufferSize_p = 10

# Buffer size (in Km) to adjust sample size of ecoregion in function of the number and distribution of legacy site
# It can be a number that will be used for all ecoregion or a file address with the specific buffer size for each ecoregion
bufferSize_N = file.path('..', 'data', 'bufferSize_N.csv')

# Distance between SSU centroid (in meters)
ssu_dist = 294

# Output folder to save the shapefiles with PSU and SSU
outputFolder = file.path('output', 'selection2023')

# suffix to add for each output layer
# e.g.: PSU-SOQB_ALL-SUFFIX.shp
fileSuffix = 'V2023'
```


## Prepare hexagons

```{r loadHex}
hexas <- readRDS(file.path('..', 'data', 'hexa_complete.RDS')) |>
    filter(propNA <= prop_na) |>
    filter(ecoregion %in% eco_sim) |>
    mutate(
        p = (hab_prob * cost_prob) / sum(hab_prob * cost_prob)
    ) |>
    filter(p != 0)
```


## legacy sites

```{r legacySites,warning=FALSE,message=FALSE}
# function to transform Latitude & longitude legacy site points in a table
# with the number of points per hexagon ID (ET_Index)
import_legacySites <- function(File, lat_name, lon_name)
{
    hx <- hexas |>
        st_transform(4326)
    
    # read file
    lg <- read_csv(File, show_col_types = FALSE) |>
        rename(
            lat = all_of(lat_name),
            lon = all_of(lon_name)
        ) |>
        st_as_sf(
            coords = c('lon', 'lat'),
            crs = st_crs(hx)
        )

    # intersect
    nbLegacy <- hx |>
        st_contains(lg, sparse = FALSE) |>
        apply(1, sum)

    tibble(
        ET_Index = hx$ET_Index,
        nbLegacySites = nbLegacy
    ) |>
    filter(nbLegacySites > 0)
}

# load and transform legacy sites (slow function)
legacySites <- import_legacySites(
    File = legacyFile,
    lat_name = lat,
    lon_name = lon
)

# merge to hexagons
hexas <- hexas |>
    left_join(legacySites) |>
    mutate(nbLegacySites = replace_na(nbLegacySites, 0))
```


## Calculate sample size given number of hexagons and legacy sites

```{r sampleSize,warning=FALSE,message=FALSE}
# define buffer size to adjust sample size
if(is.character(bufferSize_N)) {
    if(file.exists(bufferSize_N)) {
        buffSizeN <- read_csv(bufferSize_N, show_col_types = FALSE) |>
            mutate(ecoregion = as.character(ecoregion))
    }else{
        stop(paste0('File "', bufferSize_N, '" does not exist. Please check if the name is correct.'))
    }
}else if(is.numeric(bufferSize_N)){
    buffSizeN <- tibble(
        ecoregion = eco_sim,
        bufferSize = bufferSize_N
    )
}else{
    stop('Type of `bufferSize_N` must be either a numeric or a character')
}

# function to get sample size for a specific ecoregion given:
# number of hexagons, number of legacy sites, bufferSize, and sample effort
get_sampleSize <- function(eco, hx, bf_N, sample_e)
{
    # get the hexagons centroid for a ecoregion
    hexa_eco <- hx |>
        filter(ecoregion == eco) |>
        st_centroid()

    if(nrow(subset(bf_N, ecoregion == eco)) > 0) {
        # create a buffer of size BufferSize_N around legacy sites
        hexa_legacy_bf <- hexa_eco |>
            filter(nbLegacySites > 0) |>
            st_buffer(subset(bf_N, ecoregion == eco)$bufferSize * 1000) |>
            st_union()

        # Compute number of hexagons in which centroid is inside legacy buffer
        nbHexas_legacy <- hexa_eco |>
            st_intersects(hexa_legacy_bf) |>
            unlist() |>
            sum()
    }else{
        nbHexas_legacy = 0
    }

    # Compute adjusted sample size
    adj_sampleSize <- round((nrow(hexa_eco) - nbHexas_legacy) * sample_e, 0)

    # Only if ecoregion is too small, assure to sample at least two hexagons for all ecoregions
    if((nrow(hexa_eco) * sample_e) < 2 & nbHexas_legacy < 1)
        adj_sampleSize = 2

    return(adj_sampleSize)
}

sampleSize <- map_dbl(
    setNames(eco_sim, paste0('eco_', eco_sim)),
    get_sampleSize,
    hx = hexas,
    bf_N = buffSizeN,
    sample_e = sample_effort
)
```


## GRTS simulations

```{r runGRTS, warning=FALSE,message=FALSE}
# Sample size by stratum (ecoregion)
# Keep ecoregion with at least a sample of 1
Stratdsgn  <- sampleSize[sampleSize > 0]

# Keep only one ecoregion for the example
Stratdsgn <- Stratdsgn['eco_217']

# make sure that ecoregion has at least 2 times more hexagons than sample N
eco_to_remove <- hexas |>
    st_drop_geometry() |>
    group_by(ecoregion) |>
    summarise(nbHexas = n()) |>
    left_join(
        sampleSize |>
            enframe() |>
            mutate(
                ecoregion = as.character(parse_number(name)),
                sampleSize_extra = value * 2
            ) |>
            select(ecoregion, sampleSize_extra)
    ) |>
    mutate(
        diff = nbHexas - sampleSize_extra
    ) |>
    filter(diff < 0) |>
    pull(ecoregion)

Stratdsgn <- Stratdsgn[!names(Stratdsgn) %in% paste0('eco_', eco_to_remove)]

# Prepare sample frame
sampleFrame <- hexas |>
    filter(ecoregion %in% parse_number(names(Stratdsgn))) |>
    mutate(    
        eco_name = paste0('eco_', ecoregion), # to match design name
        mdcaty  = sum(Stratdsgn) * p/sum(p),
        geometry = sf::st_geometry(sf::st_centroid(geometry))
    )
    
# Coordinates of all hexagons in matrix format for MBHdesign
coord_mt <- sampleFrame |>
    st_coordinates()

# Coordinates of legacy hexagons
legacySites <- sampleFrame |>
    filter(nbLegacySites > 0) |>
    st_coordinates()

# Adjust inclusion probability of neighbour hexagons in function of legacy sites
sampleFrame$adj_p <- MBHdesign::alterInclProbs(
    legacy.sites = legacySites,
    potential.sites = coord_mt,
    inclusion.probs = sampleFrame$mdcaty,
    sigma = bufferSize_p * 1000
)

# run GRTS
grts_main <- grts_over <- list()
for(Rep in 1:nb_rep)
{
    out_sample <- spsurvey::grts(
        sframe = sampleFrame,
        n_base = Stratdsgn,
        n_over = Stratdsgn,
        stratum_var = 'eco_name',
        aux_var = 'adj_p'
    )

    grts_main[[paste0('Rep_', Rep)]] <- out_sample$sites_base$ET_Index
    grts_over[[paste0('Rep_', Rep)]] <- out_sample$sites_over$ET_Index
}

# Select cheapest replication (based on 'main' selection)
cheapest_rep <- map_df(
    grts_main,
    ~ hexas |>
        st_drop_geometry() |>
        filter(ET_Index %in% .x) |>
        summarise(totalCost = sum(costSum))
) |>
pull(totalCost) |>
which.min()

selected_main <- hexas |>
    filter(ET_Index %in% grts_main[[cheapest_rep]])
selected_over <- hexas |>
    filter(ET_Index %in% grts_over[[cheapest_rep]])
```


## Neighbor sample

Select one of the 6 neighbor hexagons with the highest inclusion probability
loop over ecoregions to make sure neighbours are from the same ecoregion

```{r neighbor, message=FALSE,warning=FALSE}
selected_extra <- hexas[0, ]
for(eco in parse_number(names(Stratdsgn)))
{
    hexas_eco <- subset(hexas, ecoregion == eco)
    hexas_sel_eco <- subset(selected_main, ecoregion == eco)

    # Extract neighbours hexagons
    neigh_mt <- hexas_eco |>
        st_centroid() |>
        st_intersects(
            y = st_buffer(hexas_sel_eco, dist = 3500),
            sparse = FALSE
        )

    # Remove the focus hexagon (the selected one)
    rr = match(hexas_sel_eco$ET_Index, hexas_eco$ET_Index)
    cc = seq_along(rr)
    neigh_mt[rr + nrow(neigh_mt) * (cc - 1)] <- FALSE

    # Select the extra hexagon based on the highest p
    best_p <- apply(
        neigh_mt,
        2,
        function(x)
            hexas_eco$ET_Index[x][which.max(hexas_eco$p[x])]
    )

    # if a selected hexagon has no neighbour, select a random from the ecoregion
    toCheck <- unlist(lapply(best_p, length))
    if(any(toCheck == 0)) {
        # which hexagons were not selected? 
        nonSelected_hexas <- setdiff(hexas_eco$ET_Index, hexas_sel_eco$ET_Index)
        
        # sample from non selected hexas
        best_p[which(toCheck == 0)] <- sample(nonSelected_hexas, sum(toCheck == 0))
    }
    
    selected_extra <- rbind(
        selected_extra,
        hexas_eco[match(best_p, hexas_eco$ET_Index), ]
    )
}
```

