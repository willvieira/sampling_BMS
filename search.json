[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Sampling design for monitoring boreal birds in Quebec",
    "section": "Welcome",
    "text": "Welcome\nThis webpage contains the technical report detailing the Sampling design for monitoring boreal birds in Quebec.\nThe online version of this report is hosted at willvieira.github.io/sampling_BMS and is automatically built whenever the code repository at github.com/sampling_BMS is modified.\n\nThe current build status is:\n\n\n\n\n\n\nThis version of the report was built on GitHub Actions on 2023-05-21."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Sampling design for monitoring boreal birds in Quebec",
    "section": "License",
    "text": "License\nTBD"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This project is part of a nationwide effort to monitor the status of birds in the underrepresented boreal region. In this report we describe the Quebec adaptation of the Boreal Optimal Sampling Strategy (BOSS). The BOSS design is a hierachical sampling approach stratified by ecoregions, habitat types, and cost constraits (Van Wilgenburg 2020). This structured design provides a spatially balanced coverage while accounting for rare habitats and sample cost. Here, we focus on the adaptation of the design for the Quebec province; for a thorough explanation and discussion of the national strategy, see Van Wilgenburg (2020).\nIn addition to stratifying the sampling based on habitat distribution and cost constraits, the BOSS design includes a function to take legacy sites and iconic sites into account. Legacy sites are existing or historical surveys with data extracted from randomly selected sites, whereas iconic sites are from non-randomly selected sites. The key reason for integrating legacy or iconic sites in the sampling design is to keep a representative sample of the community while reducing the sample cost. This is especially important in Quebec, as there are many historical data in the southern part of the province. Considering legacy sites in well-covered regions allows us to allocate ressources to remote areas with less data and higher sampling costs. In Chapter 5, we detail a novel approach accounting for the number and distribution of legacy and iconic sites to reduce sample size and maintain a representative sample of habitat types.\nOnce habitat types, cost constraints, and legacy sites are defined, the BOSS design uses the Generalized Random Tessellation Stratified Sampling (GRTS; Stevens Jr and Olsen (2004)) method to perform the random sampling. This is a widely used approach to ensure spatially balanced samples in a region. The GRTS uses a mapping function to transform two-dimensional space into one-dimensional space with an ordered spatial address. This one-dimensional ordered space is then randomly reordered before the sampling. This random reordering of the linear, one-dimensional space ensures a spatially well-balanced sample, whatever the sample size. After being sampled, this one-dimensional space is then mapped back to the original two-dimensional space.\nThis report is divided into two main sections. The first section details the spatial layers to feed the GRTS algorithm. We begin by describing the study area in the Quebec province, the selected ecoregions, and the Primary Sample Unit (PSU). We then detail de habitat and cost layers to weight the inclusion probabilities. Finally, we dedicate a complete section to describe the simulations used to create the new method to account for legacy and iconic sites. The first section contains most of the steps that have been regionalized for Quebec. The second section details the sample steps using the GRTS algorithm. In this second part, we will begin by describing the method used to calculate the stratified sample size for each of the ecoregions. We then detail the use of the GRTS to sample the PSUs and the Secondary Sample Unit (SSU).\n\n\n\n\nStevens Jr, Don L, and Anthony R Olsen. 2004. “Spatially Balanced Sampling of Natural Resources.” Journal of the American Statistical Association 99 (465): 262–78.\n\n\nVan Wilgenburg, C. Lisa AND Campbell, Steven L. AND Mahon. 2020. “A Cost Efficient Spatially Balanced Hierarchical Sampling Design for Monitoring Boreal Birds Incorporating Access Costs and Habitat Stratification.” PLOS ONE 15 (6): 1–28. https://doi.org/10.1371/journal.pone.0234494."
  },
  {
    "objectID": "studyArea.html#area-of-study",
    "href": "studyArea.html#area-of-study",
    "title": "2  Sampling frame",
    "section": "Area of study",
    "text": "Area of study\nThe study area for Quebec is outlined in Figure 2.1. It was expanded beyond the Boreal boundary to include the Arctic ecosystems. The study area contains a total of 7 ecozones, and their sizes and proportions are described in Table 2.1:\n\n\n\n\nTable 2.1: Area (in hectares) and proportion of ecozones covered by the study area.\n\n\nEcozone\nArea (% prop)\n\n\n\n\nTaiga Shield\n56275939 (35.74)\n\n\nBoreal Shield\n51342357 (32.61)\n\n\nSouthern Arctic\n27378100 (17.39)\n\n\nNorthern Arctic\n12918116 (8.2)\n\n\nHudson Plain\n6235409 (3.96)\n\n\nArctic Cordillera\n1713974 (1.09)\n\n\nAtlantic Maritime\n1587877 (1.01)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Study area (colored polygons) and its ecozone. The ecozone map was extracted from the Terrestrial Ecoregions of Canada data product - Government of Canada; Agriculture and Agri-Food Canada.\n\n\n\n\nIn order to accommodate habitat heterogeneity, the study area was hierarchically stratified into different levels of spatial aggregation. Below, we will provide a brief description of each of these strata, ranging from the ecoregion level to the specific sampling point level. For a more comprehensive explanation of the reasoning behind each stratification, please refer to Van Wilgenburg (2020)."
  },
  {
    "objectID": "studyArea.html#ecoregion",
    "href": "studyArea.html#ecoregion",
    "title": "2  Sampling frame",
    "section": "Ecoregion",
    "text": "Ecoregion\nThe ecoregion is the first level of aggregation in the sampling design. The sample size and habitat inclusion probability (described in the next chapter) are defined for each separate ecoregion. There are a total of 26 ecoregions in the study area (Figure 2.2), and their details are described in Table 2.2. Ecoregion 131 was excluded from the study area because it was too small to support enough sampling points for the random sampling design.\n\n\n\n\nTable 2.2: Area (in hectares) and proportion of ecozones covered by the study area.\n\n\nCode\nName\nArea (% prop)\n\n\n\n\n101\nCentral Laurentians\n19431698 (12.35)\n\n\n47\nCentral Ungava Peninsula\n18114106 (11.51)\n\n\n74\nNew Quebec Central Plateau\n17262947 (10.97)\n\n\n72\nLa Grande Hills\n12929820 (8.22)\n\n\n75\nUngava Bay Basin\n9671186 (6.15)\n\n\n103\nMecatina Plateau\n9388108 (5.97)\n\n\n100\nRiviere Rupert Plateau\n9083301 (5.77)\n\n\n73\nSouthern Ungava Peninsula\n8247511 (5.24)\n\n\n96\nAbitibi Plains\n6787969 (4.31)\n\n\n99\nSouthern Laurentians\n5853410 (3.72)\n\n\n48\nOttawa Islands\n5766770 (3.67)\n\n\n31\nNorthern Ungava Peninsula\n5714604 (3.63)\n\n\n28\nMeta Incognita Peninsula\n5295229 (3.37)\n\n\n217\nJames Bay Lowlands\n4482672 (2.85)\n\n\n86\nMecatina River\n2552186 (1.62)\n\n\n46\nSouthampton Island Plain\n2550507 (1.62)\n\n\n76\nGeorge Plateau\n2501318 (1.59)\n\n\n30\nWager Bay Plateau\n1908283 (1.21)\n\n\n77\nKingarutuk-Fraser River\n1861164 (1.18)\n\n\n216\nHudson Bay Lowland\n1752738 (1.11)\n\n\n7\nTorngat Mountains\n1713974 (1.09)\n\n\n117\nAppalachians\n1553719 (0.99)\n\n\n78\nSmallwood Reservoir-Michikamau\n1148044 (0.73)\n\n\n49\nBelcher Islands\n946717 (0.6)\n\n\n102\nAnticosti Island\n790375 (0.5)\n\n\n131\nIles-de-la-Madeleine\n34149 (0.02)\n\n\n\n\n\n\nThe sample size for this study was determined solely based on the size of the ecoregion. While the BOSS design considered bird species richness to increase sampling in regions with more bird species, we chose not to use this metric because it may be biased by sampling efforts in the southern region, potentially increasing sampling bias in already well-covered regions. Our goal was to sample 2% of the available hexagons (PSU described below) in each ecoregion. We defined a hexagon as available for sampling if at least 20% of it contained natural habitat types.\n\n\n\n\n\nFigure 2.2: Ecoregions (code) of the study area. Code description is detailed in Table 2.2."
  },
  {
    "objectID": "studyArea.html#primary-sampling-unit-psu",
    "href": "studyArea.html#primary-sampling-unit-psu",
    "title": "2  Sampling frame",
    "section": "Primary Sampling Unit (PSU)",
    "text": "Primary Sampling Unit (PSU)\nWe followed the BOSS design by using a 5 km diameter hexagon (Figure 2.3) as the Primary Sampling Unit (PSU). This was the lowest level of aggregation before performing the stratified sample with the GRTS algorithm. We selected only the hexagons whose centroid fell within the study area. Similarly, each hexagon was classified into one of the ecoregions using the same centroid rule. The number of hexagons, available hexagones, and sample size is described in Table 2.3.\n\n\n\n\nTable 2.3: Distribution of total hexagons, hexagons with at least 20% of natural habitats, and sample size (2%) across the ecoregions.\n\n\n\n\n\n\n\n\n\nEcoregion code\nEcoregion name\nTotal Hexagons\nAvailable Hexagons\nSample size\n\n\n\n\n7\nTorngat Mountains\n866\n866\n17\n\n\n28\nMeta Incognita Peninsula\n65\n65\n1\n\n\n30\nWager Bay Plateau\n174\n174\n3\n\n\n31\nNorthern Ungava Peninsula\n2307\n2307\n46\n\n\n46\nSouthampton Island Plain\n206\n206\n4\n\n\n47\nCentral Ungava Peninsula\n9580\n9580\n192\n\n\n48\nOttawa Islands\n36\n36\n1\n\n\n49\nBelcher Islands\n4\n4\n0\n\n\n72\nLa Grande Hills\n7490\n7490\n150\n\n\n73\nSouthern Ungava Peninsula\n5019\n5019\n100\n\n\n74\nNew Quebec Central Plateau\n10773\n10773\n215\n\n\n75\nUngava Bay Basin\n5549\n5549\n111\n\n\n76\nGeorge Plateau\n1482\n1482\n30\n\n\n77\nKingarutuk-Fraser River\n1184\n1184\n24\n\n\n78\nSmallwood Reservoir-Michikamau\n715\n715\n14\n\n\n86\nMecatina River\n1607\n1607\n32\n\n\n96\nAbitibi Plains\n4139\n4139\n83\n\n\n99\nSouthern Laurentians\n3563\n3563\n71\n\n\n100\nRiviere Rupert Plateau\n5539\n5539\n111\n\n\n101\nCentral Laurentians\n11970\n11970\n239\n\n\n102\nAnticosti Island\n487\n487\n10\n\n\n103\nMecatina Plateau\n5711\n5711\n114\n\n\n117\nAppalachians\n957\n957\n19\n\n\n131\nIles-de-la-Madeleine\n10\n10\n0\n\n\n216\nHudson Bay Lowland\n23\n23\n0\n\n\n217\nJames Bay Lowlands\n2293\n2293\n46"
  },
  {
    "objectID": "studyArea.html#secondary-sampling-unit-ssu",
    "href": "studyArea.html#secondary-sampling-unit-ssu",
    "title": "2  Sampling frame",
    "section": "Secondary Sampling Unit (SSU)",
    "text": "Secondary Sampling Unit (SSU)\nFor each PSU hexagon, we created a grid of Secondary Sampling Units (SSUs). The SSU represents the ultimate sampling locations to be utilized in the field. Instead of using the proposed 300-meter distance from the BOSS design, we followed the approach used in the Ontario regionalization design, where each SSU was separated by 294 meters. We made this small reduction in distance to ensure that the same number of SSUs were present across all PSU hexagons. Figure 2.3 displays the distribution of Secondary Sampling Units (SSUs) within a hexagon.\n\n\n\n\n\nFigure 2.3: Distribution of Secondary Sampling Units (SSUs) spaced 294 meters apart within an Primary Sampling Unit (PSU) hexagon.\n\n\n\n\n\n\n\n\nVan Wilgenburg, C. Lisa AND Campbell, Steven L. AND Mahon. 2020. “A Cost Efficient Spatially Balanced Hierarchical Sampling Design for Monitoring Boreal Birds Incorporating Access Costs and Habitat Stratification.” PLOS ONE 15 (6): 1–28. https://doi.org/10.1371/journal.pone.0234494."
  },
  {
    "objectID": "habitat.html#land-cover",
    "href": "habitat.html#land-cover",
    "title": "3  Habitat information",
    "section": "Land cover",
    "text": "Land cover\nTo control for relative frequency of habitats within each ecoregion, we used the Land cover Canada and Land cover Quebec. Both layers have habitat classes at a resolution of 30 meter pixel. The Land cover of Quebec (source?) has 10 extra classes of habitat when compared to the Land cover of Canada and is, therefore, more precise to control for habitat heterogeneity. However, the spatial extension of this layer does not cover all the study area. To avoid conflit of different sources of information within a ecoregion, the Land cover of Quebec was only used when it covered the total area of the ecoregion (Figure 3.1). For the remaining ecoregions, we used the Land cover of Canada, version 2015 (Latifovic et al. 2016). Following the BOSS design, for both Land cover layers the Snow and ice, water, Urban, and cropland classes were excluded to keep only the classes of interest for the sampling design.\n\n\n\n\n\nFigure 3.1: Ecoregions using Land cover Canada (red) or Land cover Quebec (blue)."
  },
  {
    "objectID": "habitat.html#inclusion-probability",
    "href": "habitat.html#inclusion-probability",
    "title": "3  Habitat information",
    "section": "Inclusion probability",
    "text": "Inclusion probability\nInclusion probability based on habitat type was calculated for each ecoregion individually. Within an ecoregion, it considers the number of habitats and their relative frequency. Let \\(C(i, e)\\) be the number of pixels from an ecoregion \\(e\\) that are equal to the habitat \\(i\\) and \\(\\# H\\) be the number of habitat classes. Then, the inclusion probability of a habitat within an ecoregion (\\(P_{i, e}\\)) is given by\n\\[\n    P_{i, e} = \\frac{\\#H^{-1}}{C(i, e)}\n\\]\nAs a result, the likelihood of a habitat being included decreases as the number of pixels increases. This weighted inclusion probability ensures that rare and abundant habitats are equally likely to be chosen.\nFinally, the inclusion probability of each hexagon is calculated by taking into account the inclusion probability and the relative frequency of each habitat found within the hexagon. For each hexagon \\(h\\) from an ecoregion \\(e\\), their habitat inclusion probability is calculated from all habitat types \\(i\\) following the equation:\n\\[\n    P_{habitat_{h, e}} = \\sum_{i=H_1}^{H} C(i, e) \\times P_{i, e}\n\\]"
  },
  {
    "objectID": "habitat.html#example-ecoregion-102",
    "href": "habitat.html#example-ecoregion-102",
    "title": "3  Habitat information",
    "section": "Example: Ecoregion 102",
    "text": "Example: Ecoregion 102\nTake, for instance, the frequency of pixels per habitat type for the ecoregion 102:\n\n\n\n\n\nHabitat type\nFrequency\n\n\n\n\n1\n5462771\n\n\n2\n884\n\n\n5\n102195\n\n\n6\n250493\n\n\n8\n165165\n\n\n10\n123548\n\n\n12\n32971\n\n\n13\n3762\n\n\n14\n1683501\n\n\n16\n9437\n\n\n25\n31889\n\n\n26\n1863\n\n\n27\n280462\n\n\n28\n224313\n\n\n29\n18783\n\n\n\n\n\nThe habitat 1 is the most frequent while the habitat 2 the least. Following the equation above, the inclusion probability of these two habitats are 1.220382e-08 and 7.541478e-05, respectively. This means that although habitat 1 is 6180 times more frequent than habitat 2, they are equally likely to be sampled. For a visual example, we show the relative proportion of habitat from a sample of 10 hexagons with and without inclusion probability Figure 3.2.\n\n\n\n\n\nFigure 3.2: Example of sampling 10 (2%) hexagons in ecoregion 102 with (left bar) and without (right) weighted probability accounting for habitat heterogenity. Eeach bar represents the relative proportion of habitat classes from the 10 selected hexagons.\n\n\n\n\n\n\n\n\nLatifovic, Rasim, Colin Homer, Rainer Ressl, Darren Pouliot, Sheikh Nazmul Hossain, René R Colditz, Ian Olthof, Chandra P Giri, and Arturo Victoria. 2016. “20 North American Land-Change Monitoring System.” Remote Sensing of Land Use and Land Cover, 303."
  },
  {
    "objectID": "cost.html#transport-layers-and-access-cost",
    "href": "cost.html#transport-layers-and-access-cost",
    "title": "4  Cost information",
    "section": "Transport layers and access cost",
    "text": "Transport layers and access cost\nWe used four layers of information to calculate the cheapest cost of access of a site: roads, trails, trains, and airports. Roads are used for truck transport and are mainly available in the southern part of the study area. Trails are available in more remote areas to be acessed by ATVs, but is relatively irrelevant given the amout of trails over the study area. Trails are used by ATVs to access remote areas, but their distribution overlaps with roads. For roads and trails, a buffer of size specified in Table 4.1 is defined around each line of access to create an accessible region by these methods of transport. The final access cost (AC) for a method of transport \\(x\\) is constant inside the buffer and is calculated as:\n\\[\n    AC_x = \\frac{x_{cost\\_per\\_day} \\times nb_{arus}}{x_{arus\\_per\\_crew\\_per\\_day}}\n\\]\nThe parameters are defined in Table 4.1, \\(x\\) is either roads or trails. We then rasterized the road and trail buffer polygons to a 30-meter resolution raster to calualte the minimum cost of access for each pixel across the study area.\n\n\n\n\n\nFigure 4.1: Transport layers used to calculate the access cost across the study area. Yellow for roads, green for trails, blue dots for airports, and purple for trains.\n\n\n\n\nFor transport by helicopter, we used the airport and train layers. Since train lines provide a source of fuel for the helicopters, they are classified as a pseudo-airport for the purposes of determining the cost of access. Among all airports from Quebec and Labrador, we filtered for airport classified as Aéroport, Héliport or Aérodrome. We also filtered for airports that have available fuel or were from either Hydro-Quebec or Administration Régionale Kativik.\nDifferent from roads and trails, the cost of access using helicopters was calculated at the level of the hexagon as there was little variation within a hexagon. The first step was to calculate the distance between each hexagon centroid and the closest airport or train line. Given the distance between a hexagon \\(h\\) and the closest airport or train line, the average access cost by helicopter (ACH) was defined as follows:\n\\[\n    ACH_{h} = CS_{h} + CF_{h} + CB_{h}\n\\]\nWhere \\(CS\\) is the cost of deplyoing the Autonomous recording unit (ARU) within a hexagon, \\(CF\\) is the cost of flying to the hexagon, and \\(CB\\) the cost of flying if base camp is needed. \\(CS\\) is calculated in function of the number of ARUs deployed in a hexagon, the time it takes to deploy an ARU, and the cost per hour of the helicopter:\n\\[\n    CS_h = nb_{ARUs} \\times H_{ARU} \\times CH_{ARU}\n\\]\n\\[\n    H_{ARU} = \\frac{helicopter_{hours\\_flying\\_within\\_sa\\_per\\_day}}{helicopter_{crew\\_size} \\times helicopter_{aru\\_per\\_person\\_per\\_day}}\n\\]\n\\[\n    CH_{ARU} = helicopter_{l\\_per\\_hour} \\times C_{l} + helicopter_{cost\\_per\\_hour}\n\\]\nIf the distance (\\(d\\)) between the hexagon and the nearest airport exceeds the range of the helicopter, additional flights will be necessary, increasing the cost per litre of fuel (\\(C_l\\)): \\[\n    C_{l} = \\begin{cases} helicopter_{airport\\_cost\\_per\\_l} & \\text{if } d &lt; helicopter_{max\\_km\\_from\\_base} \\\\\n    helicopter_{base\\_cost\\_per\\_l} & \\text{if } d &lt; 2 \\times helicopter_{max\\_km\\_from\\_base} \\\\\n    helicopter_{2nd\\_base\\_cost\\_per\\_l} & \\text{otherwise}\n    \\end{cases}\n\\]\nThe cost of flying (\\(CF\\)) to the hexagon from an airport is two times the distance between them, multiplied by the cost per kilometre:\n\\[\nCF_h = 2 \\times d \\times C_d\n\\]\n\\[\n    C_d = \\frac{CH_{ARU}}{helicopter_{relocation\\_speed}}\n\\]\nIn case a base camp is required due to the long distance of the hexagon, the parameter \\(d\\) from the equation above becomes the distance between the hexagon and the base camp. Then, the cost of flying from the airport to base camp (\\(CB\\)) is defined as:\n\\[\n    CB_h = d \\times helicopter_{base\\_setup\\_cost\\_per\\_km} + \\frac{2 \\times d}{helicopter_{relocation\\_speed} \\times helicopter_{l\\_per\\_hour} \\times C_l}\n\\]\n\n\n\n\nTable 4.1: Parameters define buffer, price, and crew support for the different methods of tranportation.\n\n\nParameter\nValue\n\n\n\n\nnb_arus\n5\n\n\ntruck_buffer\n1000\n\n\ntruck_cost_per_day\n600\n\n\ntruck_n_crews\n2\n\n\ntruck_arus_per_crew_per_day\n5\n\n\natv_buffer\n1000\n\n\natv_cost_per_day\n1200\n\n\natv_n_crews\n2\n\n\natv_arus_per_crew_per_day\n3\n\n\nhelicopter_cost_per_hour\n1250\n\n\nhelicopter_max_km_from_base\n150\n\n\nhelicopter_base_setup_cost_per_km\n9\n\n\nhelicopter_l_per_hour\n160\n\n\nhelicopter_crew_size\n4\n\n\nhelicopter_aru_per_person_per_day\n5\n\n\nhelicopter_relocation_speed\n180\n\n\nhelicopter_airport_cost_per_l\n1.3\n\n\nhelicopter_base_cost_per_l\n5\n\n\nhelicopter_2nd_base_cost_per_l\n10\n\n\nhelicopter_hours_flying_within_sa_per_day\n5"
  },
  {
    "objectID": "cost.html#inclusion-probability",
    "href": "cost.html#inclusion-probability",
    "title": "4  Cost information",
    "section": "Inclusion probability",
    "text": "Inclusion probability\nTo compute the inclusion probability of a hexagon in function of its accessibility, we calculated a weighted access cost based on the proportions of each access method used in the hexagon. We used the raster with the minimum cost among roads and trails to estimate the proportion of the hexagon that is accessible by land. The rest of the hexagon that is not covered by the road and trail buffer is then only accessible by helicopter. Given all available methods of transport in a hexagon, the weighted average cost is defined as the sum of the cost of each of these methods, weighted by their proportion in the hexagon:\n\\[\n    W_{\\text{averge cost}_{h}} = \\frac{\\sum_{i=1}^{n} w_{i}Cost_{i}}{\\sum_{i=1}^{n} w_i}\n\\]\nThen, for a hexagon \\(h\\) and each method of transport \\(i\\), its cost inclusion probability \\(P_{cost_{h}}\\) is given by:\n\\[\n    P_{cost_{h}} = \\frac{1}{\\sqrt{W_{\\text{averge cost}_{h}}}}\n\\]\nWhere \\(w_i\\) is the weight given by the proportion of the hexagon accessible by the method \\(i\\)."
  },
  {
    "objectID": "legacy.html#historical-sites-in-the-study-area",
    "href": "legacy.html#historical-sites-in-the-study-area",
    "title": "5  Legacy and iconic sites",
    "section": "Historical sites in the study area",
    "text": "Historical sites in the study area\nTODO: describe the different sources\n\n\n\n\n\nFigure 5.1: Legacy and historical sites from the study area."
  },
  {
    "objectID": "legacy.html#why-should-we-consider-sample-size",
    "href": "legacy.html#why-should-we-consider-sample-size",
    "title": "5  Legacy and iconic sites",
    "section": "Why should we consider sample size?",
    "text": "Why should we consider sample size?\nHere, we discuss why changing inclusion probability alone is insufficient by comparing two ecoregions with oposite spatial distributions of historical sites. While the ecoregion 99 has evenly distributed historical sites over space, the ecoregion 217 present most of its historical sites around roads. We begin by outlining the method for reducing the inclusion probability of sites near legacy sites from Foster (2021), which we then apply to the two contrasting ecoregions.\nEvery PSU hexagon is assigned an inclusion probability as a function of habitat and access cost. The effect of a legacy site (\\(l\\)) on the inclusion probability of neighbouring hexagons (\\(h\\)) is derived from the Euclidean distance between \\(h\\) and \\(l\\) (\\(d(h, l)\\)), and the amplitude effect \\(\\sigma\\) using a Gaussian function. Figure 5.2 shows the one-dimensional effect of increasing values of \\(\\sigma\\) on the inclusion probability of neighbouring sites.\n\n\n\n\n\nFigure 5.2: Legacy effect in the inclusion probabiity of neighbouring sites in function their Euclian distance. Each line represents the effect range of legacy sites. Figure adapted from the vignette ‘An Introduction to MBHdesign’ found in Foster (2021).\n\n\n\n\nThe method to reduce the inclusion probability of neighbouring sites near legacy sites developed in Foster (2021) is implemented in the R package MBHdesign. We redrew their illustration using a simulated grid landscape with three legacy sites to ilustrate the effect of increasing the \\(\\sigma\\) parameter on the inclusion probability of adjacent sites (Figure 5.3).\n\n\n\n\n\nFigure 5.3: Effect of increasing the effect (sigma) of three legacy sites (white dots) on the inclusion probability of neighbouring sites. Each square is a simulated sampling grid with different inclusion probability described by the color gradient. Inclusion probability increases from dark blue (low) to yellow (high).\n\n\n\n\nWe applied the same approach for two ecoregion with contrasting spatial distribution of historical sites. By using the spatially balanced design to sample new sites while omitting historical sites, the selected hexagons are evenly distributed throughout the ecoregion, some of which overlap with the historical sites (Figure 5.4). Adjusting the probability of inclusion of hexagons, given their distance to historical locations, increases the spatial coverage of underrepresented areas (Figure 5.5).\n\n\n\n\n\nFigure 5.4: Example of when sampling sites (red dots) using the spatially balanced design without taking historical sites into account. Ecoregion 99 and 207 are two examples of legacy sites in which the legacy sites (blue dots) are evenly and non-uniformly distributed in space, respectively.\n\n\n\n\n\n\n\n\n\nFigure 5.5: Example of when sites are sampled from a design in which only the inclusion probability is adjusted when incorporating historical sites. Ecoregion 99 and 207 are two examples of legacy sites in which the legacy sites (blue dots) are evenly and non-uniformly distributed in space, respectively.\n\n\n\n\nThe issue is that when an ecoregion is well covered by historical sites, the sampled hexagons are driven to cluster in smaller available areas. When the inclusion probability was adjusted for ecoregion 99, almost a third of the sampled hexagons were clustered in a 20 km buffer. In the next section, we will build on this approach so the sample size may also be adjusted in response to historical sites."
  },
  {
    "objectID": "legacy.html#a-new-approach-to-integrate-legacy-and-iconic-sites-in-spatially-balanced-designs",
    "href": "legacy.html#a-new-approach-to-integrate-legacy-and-iconic-sites-in-spatially-balanced-designs",
    "title": "5  Legacy and iconic sites",
    "section": "A new approach to integrate legacy and iconic sites in spatially balanced designs",
    "text": "A new approach to integrate legacy and iconic sites in spatially balanced designs\nCurrent methods to include historical surveys are limited to randomly selected sites. While they adjust the inclusion probability to avoid sampling near historical sites, there is no consideration for sample size adjustment. Here we propose a novel approach where, in addition to modifying the inclusion probability, we also adjust the sample size according to the distribution of historical sites.\n\nTheoretical description\nWe use a simulated grid with equal inclusion probabilities for all the 7661 hexagons to illustrate our approach. Given a 2% sample effort, the sample size for the model grid is 153 hexagons. We used two scenarios with contrasting spatial distributions of historical sites to show how their distribution affect sample size (Figure 5.6). The first scenario (red dots) describe evenly distributed historical sites, while the second (blue dots) describes historical sites grouped in two spatial clusters.\n\n\n\n\n\nFigure 5.6: Simulated grid with equal inclusion probabilities among the 7661 hexagons. Two scenarios of historical sites consisting of 20 sites each illustrate the effect of evenly distributed (blue) and clustered historical sites on the sample size.\n\n\n\n\nThe main idea of adjusting the inclusion probability is that the population present in the region around historical sites are well represented by the existing sample. We build on this rationale to propose that the sample size of an ecoregion can be similarly adjusted given the spatial coverage of historical sites within the ecoregion. The total coverage of a historical site can be defined by a buffer of size \\(s\\). Then, the total area of the ecoregion is subtracted by the total area covered by historical sites to determine the final sample size for an ecoregion. By adding a buffer size to the historical locations in our grid example, the balanced scenario covered 14% of the total area, whereas the unbalanced scenario only covered 7.5% (Figure 5.7). Due to their clustered distribution, the unbalanced historical sites in this example are only 50% as effective at reducing the sample size as the balanced sites. This approach hopes to solve the two issues that emerged when only inclusion probability was adjusted. First, it offers the opportunity to use both legacy and iconic sites, regardless of the randomness of their distribution. Finally, it enables us to accommodate the number and distribution of historical sites in order to adjust the sample size and avoid clustered samples in ecoregions with adequate coverage.\n\n\n\n\n\nFigure 5.7: Grid of sample units and historical sites spatially balanced (blue) and unbalanced (red) for a simulated region. Using the same approach for adjusting the inclusion probability, a buffer around each historical site is created to calculate the adjusted sampled size.\n\n\n\n\nThe unsolved issue is how to define the optimal value for the coverage of a historical site (\\(s\\)). In a perfect spatial balanced distribution of historical sites, the adjusted sample size of an ecoreregion should be reduced by the total number of historical sites. Take, for instance, an ecoregion with initial sample size of 80 hexagons. The presence of 10 spatially balanced historical sites should yields an adjusted sample size of 70 hexagons. Then, the optimal buffer size \\(s\\) must produce an adjusted sample size (\\(N_{adj}\\)) that equals the difference between the initial sample size (\\(N_{base}\\)) and the number of historical sites (\\(N_{hist}\\)), given the historical sites are spatially balanced across the ecoregion (Figure 5.8).\n\n\n\n\n\nFigure 5.8: Effect of the buffer size, determining the coverage of historical sites, on the adjusted sample size for a simulated grid with equal inclusion probabilities. Given a spatially balanced distribution of historical sites, the optimal buffer size is determined when the adjusted sample size equals the difference between the initial sample size (N base) and the number of historical sites (N hist). The color gradient around the mean black line represents the 95%, 80%, and 50% confidence intervals for 50 replications of random generated spatially balanced historical sites.\n\n\n\n\nWith this approach, we are able to determine the ideal buffer size that will allow us to reduce the sample size without compromising the spatial representation of the initial sampling effort. This simulation-based strategy, however, is sensible for the fixed number of historical sites determined a priori. In the example grid used in Figure 5.8, we simulated 20 historical sites, which represents a 0.2% of the total hexagons in the sample grid. We simulated different sets of historical sites ranging from 5 (0.07%) to 1400 (20%) to illustrate its impact on the effect of buffer size on the adjusted sample size (Figure 5.9).\n\n\n\n\n\nFigure 5.9: Effect of the buffer size on the adjusted sample size while controling for relative abundance of historical sites in the sample grid. Relative proportion of historical sites in the sample grid ranged from 0.07 to 20%.\n\n\n\n\nAs expected, increasing the proportion of historical sites increases the rate of change (slope) of the buffer size effects on the adjusted sample size. Using the same simulations, we computed the optimal buffer size for each of the proportion of historical sites (\\(\\frac{N_{hist}}{N_{total}}\\)) so the adjusted sample size (\\(N_{adj}\\)) equals the difference between the initial sample size (\\(N_{base}\\)) and the number of historical sites (\\(N_{hist}\\)) (Figure 5.10). Interestingly, increasing the proportion of historical sites increases the optimal buffer size up to a point until it reaches the sample effort (specified to 2% for the figure). The optimal buffer size reduces as the proportion of historical sites increases for proportions greater than the specified sampling effort.\n\n\n\n\n\nFigure 5.10: Optimal buffer size in function of the proportion of historical sites to the total number of hexagons. Each dot is a replication of the simulated increase of proportion of historical sites. A small amount of noise around each dot was added for visualization purposes. Blue line is smooth estimation using Generalized additive model to demonstrate the exponential effect of the proportion of historical sites on the optimal buffer size.\n\n\n\n\nTo illustrate the effect of different sampling efforts on the relationship between the proportion of historical sites and the optimal buffer size, we simulated sampling efforts ranging from 1 to 20%. (Figure 5.11).\n\n\n\n\n\nFigure 5.11: Optimal buffer size in function of the proportion of historical sites for different sampling efforts ranging from 1 to 20%. For clarity, we choose to omit the data points and illustrate only the smooth estimation using Generalized additive model.\n\n\n\n\nThe fact that the optimal buffer size changes with the number of historical sites in a region may be related to buffer coverage overlapping between historical sites and the border effects where coverage does not have effect on sample size. As a result, the optimal buffer size may also depend on the size and shape of the sample grid.However, given that the shape and size of the ecoregions are not adjustable, we decided to not test for these variables.\n\n\nResults\nFollowing the theoretical development, we performed the identical simulations using actual ecoregion boundaries rather than simulated grids. We simulated spatially balanced historical sites across the ecoregions with relative proportions to the total number of hexagons ranging from 0.2 to 12%. For each value of proportion, we calculated the adjusted sample size for different buffer sizes with radius ranging from 4 to 40 km. Ecoregions 131, 28, 30, 46, 48, 49, and 216 were excluded due to the very small size that would result in zero generated historical size for most of the simulation sets.\nThe effect of buffer size and the proportion of historical sites on the adjusted sample size using the ecoregion boundaries were similar to those performed using a simulated grid (Figure 5.12). The effect of increasing the buffer size on the reduction of the adjusted sample size is non-linear, with the rate of change increasing as the number of historical sites rises.\n\n\n\n\n\nFigure 5.12: Adjusted sample size in function of buffer size for different proportion of historical sites compared to initial sample size. When buffer size and proportion of historical sites equals zero, adjusted sample size is equal to the initial sample size.\n\n\n\n\nThis non-linear relationship can be observed in the effect of the number of historical sites on the optimal buffer size when calculating the optimal buffer size. (Figure 5.13). The optimal buffer size increases with the number of historical sites up to a point where it decreases for most ecoregions as the proportion of historical sites increases. Only the ecoregion 131 show a slight decrease in optimal buffer size with the number of historical sites.\n\n\n\n\n\nFigure 5.13: Optimal buffer size in function of the proportion of historical sites to the initial sample size. Each dot is a replication of the simulated increase of proportion of historical sites. Blue line is smooth estimation using Generalized additive model to demonstrate the exponential effect of the proportion of historical sites on the optimal buffer size.\n\n\n\n\nAcross the simulations and replications, the distribution of optimal buffer size follows a bimodal distribution.(Figure 5.14).\n\n\n\n\n\nFigure 5.14: Distribution of optimal buffer size across all simulation variables and 15 replications for each ecoregion.\n\n\n\n\n\n\nThe case of ecoregions 99 and 217\nWe apply our approach to reduce the same size in function of the number and distribution of historical sites for two contrasting ecoregions. The ecoregions 99 and 217 have 579 (16.2%) and 97 (3.5%) hexagons classified as historical sites, respectively. While the number of historical sites outweighed the sampling effort (2%), the spatial distribution of these sites varied depending on the ecoregion. Spatial distribution of historical sites was balanced in ecoregion 99, but clustered in ecoregion 217. As a result, the adjusted sample size for these two ecoregions should also be different.\nThe optimal buffer size in fuction of the proportion of historical sites for the ecoregions 99 and 217 was 19.2 and 30.8 Km, respectively. The adjusted sample size taking into account the distribution and number of historical sites was reduced from 72 to 3 for ecoregion 99, and from 56 to 22 for ecoregion 217 (Figure 5.15).\n\n\n\n\n\nFigure 5.15: Distribution of historical sites and their coverage according to the optimal buffer size specific for the ecoregion.\n\n\n\n\n\n\nLimitations\nIn order to achieve an optimal design, additional simulations may be required to fine-tune the parameters. While we have accounted for spatial coverage in adjusting the sample size, this approach could be also extended to include habitat distribution in order to better account the effect of legacy sites in the sampling design.\n\n\n\n\nFoster, Scott D. 2021. “MBHdesign: An r-Package for Efficient Spatial Survey Designs.” Methods in Ecology and Evolution 12 (3): 415–20. https://doi.org/https://doi.org/10.1111/2041-210X.13535."
  },
  {
    "objectID": "PSU.html#inclusion-probabilities",
    "href": "PSU.html#inclusion-probabilities",
    "title": "6  Primary sampling unity",
    "section": "Inclusion probabilities",
    "text": "Inclusion probabilities\nFollowing the BOSS design (Van Wilgenburg 2020), the first step in defining the hexagon inclusion probability is to merge the habitat and cost probabilities. Given they have similar importance weight, the inclusion probability of a hexagon \\(h\\) from an ecoregion \\(e\\) is defined by:\n\\[\n  P_{h, e} = \\frac{P_{habitat_{h, e}} \\times P_{cost_{h}}}{\\sum^{h_n}{(P_{habitat_{h, e}} \\times P_{cost_{h}})}}\n\\]\nWhere \\(P_{habitat}\\) is the habitat inclusion probability (Chapter 3) and \\(P_{cost}\\) is the cost inclusion probability (Chapter 4). The product of inclusion probabilities between habitat and cost layers is normalized so that it adds up to 1 across all available hexagons within the study area.\nThe final stage involves modifying the inclusion probability \\(P_{h,e}\\) for neighboring hexagons located near historical sites. To achieve this, we used the MBHdesign R package, which adjusts the inclusion probability according to a predefined bufferSize_p parameter (discuted bellow). The method employed in the R package uses a Gaussian function to reduce the inclusion probability on neighboring hexagons, with its impact diminishing as distance from the historical site increases (more details in Chapter 5)."
  },
  {
    "objectID": "PSU.html#user-paramaters",
    "href": "PSU.html#user-paramaters",
    "title": "6  Primary sampling unity",
    "section": "User paramaters",
    "text": "User paramaters\nTo run the GRTS, the first required parameter is the list of ecoregions to be included in the sampling design, which enables us to concentrate on particular ecoregions of interest. In order to ensure that the selected hexagons have natural habitats suitable for sampling, we apply a filter to keep only the hexagons with a specific proportion of non-NA pixels. The prop_na parameter is used to define this threshold, our default value was defined to 0.8 which means that a hexagon must have at least 20% of natural habitat to be available for sampling.\nAfter filtering the ecoregions and hexagons following the above rules, the next step is to determine the sample effort. As discuted in Chapter 2, we determined the sample size solely based on the size of the ecoregion, with target of sampling 2% of the available hexagons (Table 2.1). We then adjusted the sample size for each ecoregion based on the number and distribution of historical sites present in that ecoregion (Chapter 5). To do this, we need a list of historical sites and their coordinates to adjust the sample size. We also need the bufferSize_N parameter to determine the range of influence of historical sites on reducing the sample size. This parameter can be either a list of buffer sizes for each ecoregion or a single distance value applied to all ecoregions. Similarly, we used the bufferSize_p parameter to determine the range effect of the historical sites on the inclusion probability. Finally, we set the number of replications for the GRTS algorithm to run."
  },
  {
    "objectID": "PSU.html#selected-layers",
    "href": "PSU.html#selected-layers",
    "title": "6  Primary sampling unity",
    "section": "Selected layers",
    "text": "Selected layers\nWe used the grts() function available in the R package spsurvey version v5.0 or above. To run the function, we have to provide few arguments. The first argument is the sframe, which consists of the point grid and included the coordinates of the hexagon centroid. The second argument is the n_base, which specifies the sample size for each ecoregion stratum. We used the same stratum object for the n_over argument, which samples a supplementary layer of hexagons (herein called the over layer) available if any of the main hexagons were unavailable for any reason. The final two arguments are the column names for the stratum name (ecoregion code) and the inclusion probability of each hexagon point sample.\nAfter defining the arguments, we execute the GRTS function with nb_rep replications. For each replication, we compute the total cost of sampling all selected main hexagons in the study area and select the replication with the lowest cost. Since the over layer obtained by the grts sampling is randomly selected in the stratum space, we also create an additional layer of hexagons that are obligatory next to the selected main hexagons. For each selected main hexagon, we extract all neighboring hexagons and choose the one with the highest inclusion probability. If a selected main hexagon has no available neighboring hexagon, we will randomly select one within the ecoregion.\nAfter running the GRTS sampling, we obtain three layers of PSU hexagons: main, over, and extra. We export these layers of selected hexagons in two different ways. The first one is a single shapefile for each layer of selected hexagons. The second way is shared the three layers of selected hexagons into a folder specific to the ecoregion. After defining the output folder to save, and the prefix arguments, the final tree of exported files follows:\nOnce the GRTS sampling is completed, we obtain three layers of selected PSU hexagons called main, over, and extra, plus the layer with all available hexagons of the study area called ALL. We export these layers of selected hexagons in two different ways. The first way is to export each layer as a separate shapefile and all ecoregion together. The second way, the three layers of selected hexagons are shared into a folder dedicated to the specific ecoregion. To export these files, we need to define the output folder to save the files and the prefix arguments to be added at the end of each shapefile. For instance, let’s define the parameter outputFolder to selection and the parameter fileSuffix to V2023. The exported PSU file tree for the hypothetical ecoregions 101, 102, 10`, and 104 will have the following format:\n\n\n/tmp/Rtmpk676gT/selection\n├── allEcoregion\n│   ├── PSU-SOQB_ALL-V2023.shp\n│   ├── PSU-SOQB_extra-V2023.shp\n│   ├── PSU-SOQB_main-V2023.shp\n│   └── PSU-SOQB_over-V2023.shp\n└── byEcoregion\n    ├── ecoregion_101\n    │   ├── PSU-SOBQ_eco101_ALL-V2023.shp\n    │   ├── PSU-SOBQ_eco101_extra-V2023.shp\n    │   ├── PSU-SOBQ_eco101_main-V2023.shp\n    │   └── PSU-SOBQ_eco101_over-V2023.shp\n    ├── ecoregion_102\n    │   ├── PSU-SOBQ_eco102_ALL-V2023.shp\n    │   ├── PSU-SOBQ_eco102_extra-V2023.shp\n    │   ├── PSU-SOBQ_eco102_main-V2023.shp\n    │   └── PSU-SOBQ_eco102_over-V2023.shp\n    ├── ecoregion_103\n    │   ├── PSU-SOBQ_eco103_ALL-V2023.shp\n    │   ├── PSU-SOBQ_eco103_extra-V2023.shp\n    │   ├── PSU-SOBQ_eco103_main-V2023.shp\n    │   └── PSU-SOBQ_eco103_over-V2023.shp\n    └── ecoregion_104\n        ├── PSU-SOBQ_eco104_ALL-V2023.shp\n        ├── PSU-SOBQ_eco104_extra-V2023.shp\n        ├── PSU-SOBQ_eco104_main-V2023.shp\n        └── PSU-SOBQ_eco104_over-V2023.shp\n\n\n\n\n\n\nVan Wilgenburg, C. Lisa AND Campbell, Steven L. AND Mahon. 2020. “A Cost Efficient Spatially Balanced Hierarchical Sampling Design for Monitoring Boreal Birds Incorporating Access Costs and Habitat Stratification.” PLOS ONE 15 (6): 1–28. https://doi.org/10.1371/journal.pone.0234494."
  },
  {
    "objectID": "SSU.html",
    "href": "SSU.html",
    "title": "7  Secondary sampling unity",
    "section": "",
    "text": "In this section we provide a detail our approach for selecting the Secondary Sampling Units (SSUs). Unlike the PSU sampling, the SSU sampling approach differs significantly from the BOSS design. For each selected hexagon, we create a grid of SSU points spaced 294 meters apart to perfrom the sampling (Figure 7.1). The chosen spacing value was optimized using the Ontario regionalization method to ensure that each hexagon contains an equal number of SSUs.\n\n\n\n\n\nFigure 7.1: The green dots represent the Secondary Sampling Units (SSUs), which are spatially separated from each other by a distance of 294 meters. A buffer with a diameter of 147 meters was created around each SSU to extract the habitat pixels.\n\n\n\n\nFor each SSU point within the PSU, we generate a buffer with a distance of 147 meters. We then extract the habitat pixels to compute the inclusion probability of each SSU point following the same approach used for the PSU (Chapter 3). This process allows us to calculate the habitat inclusion probability and the proportion of non-empty habitats pixels for each SSU hexagon.\nThe final Step in the process of prepareing the SSU for sampling is to assign a matrix of neighouring SSU points for each focal SSU. This is important because each SSU is classified as available to be sampled in function of its proportion of non-empty habitats pixels, the number of neighbours SSU, and the proportion of non-empty habitats pixels of the neighbours SSUs. Specifically, for a SSU to be available for sampling it must have:\nThe next step is to assign a matrix of neighboring SSU points for each focal SSU. This is crucial because the availability of each SSU for sampling is determined based on its proportion of non-empty habitat pixels, the number of neighboring SSUs, and the proportion of non-empty habitat pixels of those neighbors. Specifically, to be considered available for sampling, a SSU must meet the following criteria:\n\nHave exactly 6 neighbors (fewer indicates being on the border of the PSU hexagon)\nPossess at least 80% non-empty pixels (prop_na = 0.8), similar to the PSU sampling\nHave at least 4 out of the 6 neighboring SSUs with at least 80% non-empty pixels\n\nAfter defining the availability of each SSU, we proceed with a random sampling of SSU points, weighted by their habitat inclusion probability. The SSU sampling is performed sequentially, meaning that for each SSU selected within a PSU hexagon, its six neighbors are automatically marked as unavailable for the next sampling round. This procedure prevents the clustering of SSU samples in a single region. The SSU sampling continues until it reaches the desired SSU sample size (defined by ssu_N), or until there are no more available SSUs remaining. The selected SSU points, along with their respective neighbors, are classified using the \\(AB\\) code pattern (Figure 7.2). Here, \\(A\\) represents the SSU sample ID, ranging from 1 to ssu_N, while \\(B\\) represents the neighbor ID, ranging from 0 to 6. In this pattern, 0 denotes the focal point, while 1 to 6 represent the six neighbors, starting from the North point and moving clockwise.\n\n\n\n\n\nFigure 7.2: Selected SSU points (green) and their respectice neighboring (yellow).\n\n\n\n\nOnce the sampling process is completed, we obtain multiple SSU points for each hexagon from one of the three layers of selected PSU hexagons: main, over, or extra. We export both all SSU points and the selected SSU points for each selected hexagon from the three PSU layers. The export of SSU points follows a similar approach to the export of PSU hexagons. We first export each layer as a separate shapefile, including all ecoregions together, and then create separate shapefiles for each ecoregion. Using the same export parameters utilized in the PSU sampling, the SSU shapefiles will be exported as follows:\n\n\n/tmp/RtmpHtqxrR/selection\n├── allEcoregion\n│   ├── SSU-SOQB_ALL_extra-V2023.shp\n│   ├── SSU-SOQB_ALL_main-V2023.shp\n│   ├── SSU-SOQB_ALL_over-V2023.shp\n│   ├── SSU-SOQB_selected_extra-V2023.shp\n│   ├── SSU-SOQB_selected_main-V2023.shp\n│   └── SSU-SOQB_selected_over-V2023.shp\n└── byEcoregion\n    ├── ecoregion_101\n    │   ├── SSU-SOBQ_eco101_ALL_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco101_ALL_main-V2023.shp\n    │   ├── SSU-SOBQ_eco101_ALL_over-V2023.shp\n    │   ├── SSU-SOBQ_eco101_selected_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco101_selected_main-V2023.shp\n    │   └── SSU-SOBQ_eco101_selected_over-V2023.shp\n    ├── ecoregion_102\n    │   ├── SSU-SOBQ_eco102_ALL_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco102_ALL_main-V2023.shp\n    │   ├── SSU-SOBQ_eco102_ALL_over-V2023.shp\n    │   ├── SSU-SOBQ_eco102_selected_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco102_selected_main-V2023.shp\n    │   └── SSU-SOBQ_eco102_selected_over-V2023.shp\n    ├── ecoregion_103\n    │   ├── SSU-SOBQ_eco103_ALL_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco103_ALL_main-V2023.shp\n    │   ├── SSU-SOBQ_eco103_ALL_over-V2023.shp\n    │   ├── SSU-SOBQ_eco103_selected_extra-V2023.shp\n    │   ├── SSU-SOBQ_eco103_selected_main-V2023.shp\n    │   └── SSU-SOBQ_eco103_selected_over-V2023.shp\n    └── ecoregion_104\n        ├── SSU-SOBQ_eco104_ALL_extra-V2023.shp\n        ├── SSU-SOBQ_eco104_ALL_main-V2023.shp\n        ├── SSU-SOBQ_eco104_ALL_over-V2023.shp\n        ├── SSU-SOBQ_eco104_selected_extra-V2023.shp\n        ├── SSU-SOBQ_eco104_selected_main-V2023.shp\n        └── SSU-SOBQ_eco104_selected_over-V2023.shp"
  },
  {
    "objectID": "runGRTSexample.html#setup",
    "href": "runGRTSexample.html#setup",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Setup",
    "text": "Setup\nBellow we list the packages used in the sampling process. Note that the code described in the report is only compatible with spsurvey R package versions v5.0 and above. For consistent results, we recommend loading the same environment that was used in this project, which is saved in the renv.lock file. The following steps can be used to load the same environment:\n\n\nCode\n# install `renv` package if necessary\nif (!require(renv)) install.packages('renv')\n# Restore the project environment\nrenv::activate(project = \"/path/to/project\")\n\n\nThe first line of code will check load (and install if necessary) the R pacakge renv to manage the version of the dependences. The activate() function will install all necessary packages with their respective version into a local container accessible only by this project. If you’re not already in the path of the project, make sure to replace /path/to/project with the path to the directory where the project is stored.\n\n\nCode\nlibrary(raster)\nlibrary(exactextractr)\nlibrary(tidyverse)\nlibrary(spatstat)\nlibrary(sf)\nlibrary(spsurvey)\nlibrary(MBHdesign)\n\n# to reproduce the same results\nset.seed(0.0)"
  },
  {
    "objectID": "runGRTSexample.html#custom-parameters",
    "href": "runGRTSexample.html#custom-parameters",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Custom parameters",
    "text": "Custom parameters\nHere is the main section the user will need to interact. Below is a list of all the variables that can be used to customize the sampling process, each accompanied by detailed comments.\n\n\nCode\n# Threshold to determine whether a hexagon is suitable for sampling is based on\n# the proportion of NA pixels (non-natural habitat).\n# `0.8` means a hexagon has to have at least 20% of habitat pixels to be\n# suitable for sampling\nprop_na = 0.8\n\n# Sample effort in percentage\nsample_effort = 0.02\n\n# Total sample size for SSU within a hexagon (Main + Over)\n# It must be an even numbers\nssu_N = 6\n\n# Number of replications when selecting the PSU with the GRTS algorithm\nnb_rep = 15\n\n# Code of ecoregions to sample\neco_sim = c(\n    '7', '28', '30', '31',\n    '46', '47', '48', '49',\n    '73', '77', '78', '86',\n    '72', '74', '75', '76',\n    '96', '99', '100', '101',\n    '102', '103', '117',\n    '216', '217'\n)\n\n# File path of the csv file used to store the legacy sites\n# E.g. https://github.com/willvieira/sampling_BMS/blob/main/data/legacySites.csv\n# `lat` and `lon` arguments are used to define the character name of the\n# columns in the csv\nand columns to extract legacy sites\nlegacyFile = file.path('..', 'data', 'legacySites.csv')\nlat = 'latitude'\nlon = 'longitude'\n\n# Buffer size (in Km) to adjust inclusion probability of hexagons around the\n# legacy sites using the MBHdesign R package\nbufferSize_p = 10\n\n# Buffer size (in Km) to adjust the sample size of an ecoregion as a function of\n# the number and distribution of its legacy sites\n# The value provided can either be a single number applied to all ecoregions or\n# a file path containing the buffer size information for each ecoregion.\n# E.g.https://github.com/willvieira/sampling_BMS/blob/main/data/bufferSize_N.csv\nbufferSize_N = file.path('..', 'data', 'bufferSize_N.csv')\n# If you want to assign the same buffer size (in Km) for all ecoregion:\n# bufferSize_N = 15\n\n# Distance between SSU centroid (in meters)\nssu_dist = 294\n\n# Path to save the shapefiles with the selected PSU and SSU\noutputFolder = file.path('output', 'selection2023')\n\n# suffix to add for each output layer\n# e.g.: PSU-SOQB_ALL-SUFFIX.shp\nfileSuffix = 'V2023'"
  },
  {
    "objectID": "runGRTSexample.html#prepare-layers-for-sampling",
    "href": "runGRTSexample.html#prepare-layers-for-sampling",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Prepare layers for sampling",
    "text": "Prepare layers for sampling\nThe first step is loading the complete hexagons list within the study area. While loading, we keep only the hexagons for the ecoregions of interest (eco_sim) and remove the hexagons with too many NA habitats. We then compute the hexagon inclusion probability from the habitat and cost layers normalized for the study area.\n\n\nCode\nhexas &lt;- readRDS(file.path('..', 'data', 'hexa_complete.RDS')) |&gt;\n    filter(propNA &lt;= prop_na) |&gt;\n    filter(ecoregion %in% eco_sim) |&gt;\n    mutate(\n        p = (hab_prob * cost_prob) / sum(hab_prob * cost_prob)\n    ) |&gt;\n    filter(p != 0)\n\n\nThe legacy sites are the second layer of information needed to run the GRTS. Bellow the code is written to load the list of coordinate points from the legacy sites and create a data frame describing the count number of legacy sites per hexagon. This procedure scales the point data to the hexagon level, allowing us to modify the inclusion probability of the neighboring hexagons and adjust the ecoregion sample size.\n\n\nCode\n# function to transform Latitude & longitude legacy site points in a table\n# with the number of points per hexagon ID (ET_Index)\nimport_legacySites &lt;- function(File, lat_name, lon_name)\n{\n    # transform hexagons projection to the same of the legacy points\n    hx &lt;- hexas |&gt;\n        st_transform(4326)\n    \n    # read legacy csv file\n    lg &lt;- read_csv(File, show_col_types = FALSE) |&gt;\n        rename(\n            lat = all_of(lat_name),\n            lon = all_of(lon_name)\n        ) |&gt;\n        st_as_sf(\n            coords = c('lon', 'lat'),\n            crs = st_crs(hx)\n        )\n\n    # intersect legacy points with hexagon polygons\n    nbLegacy &lt;- hx |&gt;\n        st_contains(lg, sparse = FALSE) |&gt;\n        apply(1, sum)\n\n    # Return a transformed data to data.frame\n    # and keep only the hexagons that contains legacy sites\n    tibble(\n        ET_Index = hx$ET_Index,\n        nbLegacySites = nbLegacy\n    ) |&gt;\n    filter(nbLegacySites &gt; 0)\n}\n\n# load and transform legacy sites (slow function)\nlegacySites &lt;- import_legacySites(\n    File = legacyFile,\n    lat_name = lat,\n    lon_name = lon\n)\n\n# merge legacy sites data.frame into hexagon object\nhexas &lt;- hexas |&gt;\n    left_join(legacySites) |&gt;\n    mutate(nbLegacySites = replace_na(nbLegacySites, 0))"
  },
  {
    "objectID": "runGRTSexample.html#adjust-sample-size-and-inclusion-probability-as-a-function-of-legacy-sites",
    "href": "runGRTSexample.html#adjust-sample-size-and-inclusion-probability-as-a-function-of-legacy-sites",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Adjust sample size and inclusion probability as a function of legacy sites",
    "text": "Adjust sample size and inclusion probability as a function of legacy sites\nAfter merging the hexagons and legacy sites, the next step is to adjust the sample size and modify the inclusion probabilities. The sample size, defined in the number of hexagons, is defined in function of the size of the ecoregion, the sampling effort (sample_effort), and the number of legacy sites. The input bufferSize_N quantify the effect of each legacy site in reducing the final sample size. The value provided can either be a single number that will be applied to all ecoregions or a file path that contains the buffer size information for each individual ecoregion. For more details on defining the buffer size to adjust the sample size, please refer to Chapter 5.\n\n\nCode\n# function to get sample size for a specific ecoregion given:\n# number of hexagons, number of legacy sites, bufferSize, and sample effort\nget_sampleSize &lt;- function(eco, hx, bf_N, sample_e)\n{\n    # get the hexagons centroid for a specific ecoregion\n    hexa_eco &lt;- hx |&gt;\n        filter(ecoregion == eco) |&gt;\n        st_centroid()\n\n    if(nrow(subset(bf_N, ecoregion == eco)) &gt; 0) {\n        # create a buffer of size `BufferSize_N` around the legacy site centroid\n        hexa_legacy_bf &lt;- hexa_eco |&gt;\n            filter(nbLegacySites &gt; 0) |&gt;\n            st_buffer(subset(bf_N, ecoregion == eco)$bufferSize * 1000) |&gt;\n            st_union()\n\n        # Compute the number of hexagons from the ecoregion in which their\n        # centroid falls within the buffer around the legacy sites\n        nbHexas_legacy &lt;- hexa_eco |&gt;\n            st_intersects(hexa_legacy_bf) |&gt;\n            unlist() |&gt;\n            sum()\n    }else{\n        nbHexas_legacy = 0\n    }\n\n    # Compute the adjusted sample size using only the total amount of hexagons\n    # that do not fall within the legacy buffer\n    adj_sampleSize &lt;- round((nrow(hexa_eco) - nbHexas_legacy) * sample_e, 0)\n\n    # If the ecoregion is too small to accommodate 2 hexagons with the defined\n    # sampling effort, ensure to sample at least two hexagons.\n    if((nrow(hexa_eco) * sample_e) &lt; 2 & nbHexas_legacy &lt; 1)\n        adj_sampleSize = 2\n\n    return(adj_sampleSize)\n}\n\n\n\n\nCode\n# Load buffer size information regardless of the `byfferSize_N` class\nif(is.character(bufferSize_N)) {\n    if(file.exists(bufferSize_N)) {\n        buffSizeN &lt;- read_csv(bufferSize_N, show_col_types = FALSE) |&gt;\n            mutate(ecoregion = as.character(ecoregion))\n    }else{\n        stop(\n            paste0('File \"', bufferSize_N,\n            '\" does not exist. Please check if the name is correct.')\n        )\n    }\n}else if(is.numeric(bufferSize_N)){\n    buffSizeN &lt;- tibble(\n        ecoregion = eco_sim,\n        bufferSize = bufferSize_N\n    )\n}else{\n    stop('The type of `bufferSize_N` must be either numeric or a character')\n}\n\n\n\n\nCode\n# Run the function to define sample size for all selected ecoregions\nsampleSize &lt;- map_dbl(\n    setNames(eco_sim, paste0('eco_', eco_sim)),\n    get_sampleSize,\n    hx = hexas,\n    bf_N = buffSizeN,\n    sample_e = sample_effort\n)\n\n# Define the stratum object with ecoregion that have\n# at least a sample size of 1\nStratdsgn  &lt;- sampleSize[sampleSize &gt; 0]\n\n# Because we are sampling an extra hexagon for each selected one,\n# make sure that each ecoregion has at least 2 times more available\n# hexagons than sample N\neco_to_remove &lt;- hexas |&gt;\n    st_drop_geometry() |&gt;\n    group_by(ecoregion) |&gt;\n    summarise(nbHexas = n()) |&gt;\n    left_join(\n        sampleSize |&gt;\n            enframe() |&gt;\n            mutate(\n                ecoregion = as.character(parse_number(name)),\n                sampleSize_extra = value * 2\n            ) |&gt;\n            select(ecoregion, sampleSize_extra)\n    ) |&gt;\n    mutate(\n        diff = nbHexas - sampleSize_extra\n    ) |&gt;\n    filter(diff &lt; 0) |&gt;\n    pull(ecoregion)\n\nStratdsgn &lt;- Stratdsgn[!names(Stratdsgn) %in% paste0('eco_', eco_to_remove)]\n\n# Prepare sample frame to be used in GRTS\n# scale inclusion probability to the total sample size\n# Use only the centroid of the hexagon as a sample point\nsampleFrame &lt;- hexas |&gt;\n    filter(ecoregion %in% parse_number(names(Stratdsgn))) |&gt;\n    mutate(    \n        eco_name = paste0('eco_', ecoregion), # to match design name\n        mdcaty  = sum(Stratdsgn) * p/sum(p),\n        geometry = sf::st_geometry(sf::st_centroid(geometry))\n)   \n\n\nThe last step involves adjusting inclusion probabilities for neighboring hexagons around legacy sites. To do this, we used the MBHdesign R package and adjusted the inclusion probability based on the bufferSize_p parameter.\n\n\nCode\n# Coordinates of all hexagons in matrix format for MBHdesign\ncoord_mt &lt;- sampleFrame |&gt;\n    st_coordinates()\n\n# Coordinates of legacy hexagons\nlegacySites &lt;- sampleFrame |&gt;\n    filter(nbLegacySites &gt; 0) |&gt;\n    st_coordinates()\n\n# Adjust inclusion probability of neighbour hexagons in function of legacy sites\nsampleFrame$adj_p &lt;- MBHdesign::alterInclProbs(\n    legacy.sites = legacySites,\n    potential.sites = coord_mt,\n    inclusion.probs = sampleFrame$mdcaty,\n    sigma = bufferSize_p * 1000\n)"
  },
  {
    "objectID": "runGRTSexample.html#primary-sampling-unit-psu",
    "href": "runGRTSexample.html#primary-sampling-unit-psu",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Primary Sampling Unit (PSU)",
    "text": "Primary Sampling Unit (PSU)\nThe spsurvey R package was used to implement the GRTS algorithm for sampling the PSUs of each ecoregion stratum. The sample size for each stratum, as defined in Stratdsn, is used to sample the hexagon points from the sampleFrame object. Note that the same sample size stratum is used for sampling the replacement (n_over) sites for each ecoregion stratum. Within each stratum, the sample selection is weighted based on the inclusion probability adj_p, which is derived from the habitat, cost, and legacy site layers. We replicate this sampling process nb_rep times, sum the total cost of sampling for the selected hexagons, and choose the replication with the lowest cost.\n\n\nCode\n# list to store the hexagons ID (ET_Index) for the main and the replacement\ngrts_main &lt;- grts_over &lt;- list()\n# Run GRTS selection over nb_rep times\nfor(Rep in 1:nb_rep)\n{\n    out_sample &lt;- spsurvey::grts(\n        sframe = sampleFrame,\n        n_base = Stratdsgn,\n        n_over = Stratdsgn,\n        stratum_var = 'eco_name',\n        aux_var = 'adj_p'\n    )\n\n    grts_main[[paste0('Rep_', Rep)]] &lt;- out_sample$sites_base$ET_Index\n    grts_over[[paste0('Rep_', Rep)]] &lt;- out_sample$sites_over$ET_Index\n}\n\n# Select cheapest replication (based on 'main' selection)\ncheapest_rep &lt;- map_df(\n    grts_main,\n    ~ hexas |&gt;\n        st_drop_geometry() |&gt;\n        filter(ET_Index %in% .x) |&gt;\n        summarise(totalCost = sum(costSum))\n) |&gt;\npull(totalCost) |&gt;\nwhich.min()\n\n# Save selected main and replacement hexagons\nselected_main &lt;- hexas |&gt;\n    filter(ET_Index %in% grts_main[[cheapest_rep]])\nselected_over &lt;- hexas |&gt;\n    filter(ET_Index %in% grts_over[[cheapest_rep]])\n\n\nSince the replacement hexagons are randomly selected in the stratum space, we also selected an extra layer of replacement hexagons that are obligatory neighbors of each main hexagon. For each selected main hexagon, we extract all neighboring hexagons and select the one with the highest inclusion probability. If a selected main hexagon has no available neighbor hexagon, we will select a random one over the ecoregion.\n\n\nCode\n# object to store extra layer of selected hexagons\nselected_extra &lt;- hexas[0, ]\n\n# loop over ecoregions to make sure neighbours are from the same ecoregion\nfor(eco in parse_number(names(Stratdsgn)))\n{\n    hexas_eco &lt;- subset(hexas, ecoregion == eco)\n    hexas_sel_eco &lt;- subset(selected_main, ecoregion == eco)\n\n    # Extract neighbours hexagons\n    neigh_mt &lt;- hexas_eco |&gt;\n        st_centroid() |&gt;\n        st_intersects(\n            y = st_buffer(hexas_sel_eco, dist = 3500),\n            sparse = FALSE\n        )\n\n    # Remove the focus hexagon (the selected one)\n    rr = match(hexas_sel_eco$ET_Index, hexas_eco$ET_Index)\n    cc = seq_along(rr)\n    neigh_mt[rr + nrow(neigh_mt) * (cc - 1)] &lt;- FALSE\n\n    # Select the extra hexagon based on the highest p\n    best_p &lt;- apply(\n        neigh_mt,\n        2,\n        function(x)\n            hexas_eco$ET_Index[x][which.max(hexas_eco$p[x])]\n    )\n\n    # if a selected hexagon has no neighbour, select a random from the ecoregion\n    toCheck &lt;- unlist(lapply(best_p, length))\n    if(any(toCheck == 0)) {\n        # which hexagons were not selected? \n        nonSelected_hexas &lt;- setdiff(hexas_eco$ET_Index, hexas_sel_eco$ET_Index)\n        \n        # sample from non selected hexas\n        best_p[which(toCheck == 0)] &lt;- sample(\n            nonSelected_hexas, sum(toCheck == 0)\n        )\n    }\n    \n    selected_extra &lt;- rbind(\n        selected_extra,\n        hexas_eco[match(best_p, hexas_eco$ET_Index), ]\n    )\n}"
  },
  {
    "objectID": "runGRTSexample.html#secondary-sampling-unit-ssu",
    "href": "runGRTSexample.html#secondary-sampling-unit-ssu",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Secondary Sampling Unit (SSU)",
    "text": "Secondary Sampling Unit (SSU)\nFor each of the selected PSU hexagons (main, over, and extra), we generate a grid of SSU points equally spaced with a distance defined by ssu_dist. The following function to generate SSUs is from the Ontario’s sampling approach (code source).\n\n\nCode\ngenSSU &lt;- function(h, spacing)\n{\n    ch &lt;- as_tibble(st_coordinates(h))\n    top_point &lt;- ch[which.max(ch$Y),]\n    bottom_point &lt;- ch[which.min(ch$Y),]\n    gridsize &lt;- 2*floor(abs(top_point$Y-bottom_point$Y)/spacing)+3\n    rowAngle &lt;- tanh((top_point$X-bottom_point$X)/(top_point$Y-bottom_point$Y))\n\n    cent &lt;- st_centroid(h) %&gt;%\n        bind_cols(as_tibble(st_coordinates(.))) %&gt;%\n        st_drop_geometry %&gt;%\n        dplyr::select(ET_Index, X, Y)\n\n    genRow &lt;- function(cX, cY, sp,...){\n        tibble(rowid = seq(-gridsize,gridsize)) %&gt;%\n        mutate(X = sin(60*pi/180+rowAngle) *sp*rowid + {{cX}},\n                Y = cos(60*pi/180+rowAngle) *sp*rowid  + {{cY}})\n    }\n\n    centroids &lt;- tibble(crowid=seq(-gridsize,gridsize)) %&gt;%\n        mutate(cY = cos(rowAngle) *spacing*crowid + cent$Y,\n            #spacing/2*crowid + cent$Y,\n            cX =  sin(rowAngle) *spacing*crowid + cent$X) %&gt;%\n        #cent$X + crowid* sqrt(spacing**2-(spacing/2)**2)) %&gt;%\n        rowwise() |&gt;\n        mutate(row = list(genRow(cX = cX,cY = cY,sp = spacing))) %&gt;%\n        unnest(row) |&gt;\n        dplyr::select(X,Y) %&gt;%\n        st_as_sf(coords = c(\"X\", \"Y\"), crs = st_crs(h)) %&gt;%\n        st_filter(h) %&gt;%\n        mutate(\n            ET_Index = h$ET_Index,\n            ecoregion = h$ecoregion,\n            ssuID = row_number()\n        )\n    return(centroids)\n}\n\n\nThe following step involves defining a function to sample the SSUs within each selected PSU hexagon. SSU sampling is done sequentially, so for each selected SSU, the first and second layers of neighbour points (6 and 12, respectively) are made unavailable for sampling the next SSU. We repeat this process until we reach the total sampling size (ssu_N) or when there are no more available SSUs left to sample.\n\n\nCode\nsample_SSU &lt;- function(ssuid, prob, geom, filtered, ssuDist, N)\n{\n    # check if N is even\n    if(N %% 2 != 0)\n        stop('`ssu_N` must be a even number.')\n\n    filtered_1 &lt;- filtered\n    out &lt;- rep(0, length(filtered))\n\n    # loop to sample N SSUs\n    count = 1\n    while(\n        count &lt;= N &\n        sum(get(paste0('filtered_', count))) &gt; 1\n    ){\n        # sample point\n        assign(\n            paste0('sample_', count),\n            sample(\n                ssuid[get(paste0('filtered_', count))],\n                size = 1,\n                prob = prob[get(paste0('filtered_', count))]\n            )\n        )\n\n        # remove points around the first sample for second point\n        toKeep &lt;- !st_intersects(\n            geom,\n            st_buffer(\n                geom[which(get(paste0('sample_', count)) == ssuid)],\n                dist = ssuDist * 2 + ssuDist * 0.1),\n                sparse = FALSE\n        )[, 1]\n\n        # update available points\n        assign(\n            paste0('filtered_', count + 1),\n            get(paste0('filtered_', count)) & toKeep\n        )\n\n        # assign point to output vector\n        out[get(paste0('sample_', count))] = count\n        \n        count = count + 1\n    }\n    return( out )\n}\n\n\nWith these two functions, we can loop over the main, over, and extra selected hexagons to create and sample SSU points. First, we create the SSU points for all selected hexagons. Then, we generate a buffer around each SSU point with a distance of ssu_dist/2. We use the habitat pixels within each buffer to calculate the inclusion probability and the proportion of NA pixels. To save computation time, we compute and store the six neighbour points around each SSU point. Before sampling the SSU points, we classify each point as available or not following these rules:\n\nit must have 6 neighbours (less than that means it’s a border SSU)\nit must have at least 1 - prop_na of non-empty pixels\n4 out of 6 neighbours must also meet these rules\n\nIn the final step, we assign a specific code to each selected SSU point and its neighbors, following the A_B pattern, where A represents the SSU sample ID (ranging from 1 to ssu_N), and B represents the neighbor ID (ranging from 0 to 6). Here, 0 represents the focal point, and 1 to 6 represent the six neighbors, starting from the top point and moving clockwise. please refer to Chapter 7 for more details.\n\n\nCode\n# Habitat shapefile to calculate inclusion probability\nland_ca &lt;- raster(file.path('..', 'data', 'landcover_ca_30m.tif'))\n# Distribution of habitat types per ecoregion\nprev_all &lt;- readRDS(file.path('..', 'data', 'prev_all.RDS'))\n\nfor(lyr in c('main', 'over', 'extra'))\n{\n    sel_lyr &lt;- get(paste0('selected_', lyr))\n\n    # Generate SSU points\n    SSUs &lt;- map_dfr(\n        seq_len(nrow(sel_lyr)),\n        ~ genSSU(sel_lyr[.x, ], spacing = ssu_dist)\n    )\n\n    # Buffer of half `ssu_dist` to compute habitat inclusion prob\n    SSU_bf &lt;- st_buffer(SSUs, dist = ssu_dist/2)\n\n    # extract pixels for each SSU polygon\n    hab_pixels &lt;- exactextractr::exact_extract(\n        land_ca,\n        SSU_bf,\n        progress = FALSE\n    )\n    rm(SSU_bf)\n\n    # get frequence of each class of habitat\n    count_hab &lt;- Map(\n                function(x, y) {\n                    freq &lt;- table(x$value)\n                    if(length(freq) &gt; 0) {\n                        data.frame(freq, ecoregion = y)\n                    }else{\n                        NA\n                    }\n                },\n                x = hab_pixels,\n                y = SSUs$ecoregion\n            )\n\n    # merge with inclusion probability\n    # and calculate inclusion probability for each NON empty polygon\n    SSUs$incl_prob &lt;- unlist(\n            lapply(\n                count_hab,\n                function(x) {\n                    if(is.data.frame(x)) {\n                        mg_df &lt;- merge(\n                            x,\n                            subset(\n                                prev_all, ID_ecoregion == x$ecoregion[1]\n                            ),\n                            by.x = \"Var1\",\n                            by.y = \"code\",\n                            all.x = TRUE\n                        )\n                        sum(mg_df$Freq * mg_df$incl_prob)\n                    }else{\n                        NA\n                    }\n                }\n            )\n        )\n    rm(count_hab)\n\n    SSUs &lt;- SSUs |&gt;\n        group_by(ET_Index) |&gt;\n        mutate(\n            incl_prob = incl_prob/sum(incl_prob, na.rm = TRUE)\n        ) |&gt;\n        ungroup()\n\n    # Calculate proportion of NA\n    SSUs$propNA &lt;- map_dbl(\n        hab_pixels,\n        ~ sum(is.na(.x$value))/nrow(.x)\n    )\n    rm(hab_pixels)\n\n    # neighbours matrix\n    neighbour_ls &lt;- list()\n    for(hx in unique(SSUs$ET_Index))\n    {\n        ssuhx &lt;- subset(SSUs, ET_Index == hx)\n\n        neighbour_ls[[hx]] &lt;- st_intersects(\n            ssuhx,\n            st_buffer(ssuhx, dist = ssu_dist + ssu_dist * 0.1),\n            sparse = FALSE\n        )\n    }\n\n    # These are the following rules to a SSU be suitable for sampling:\n    # - Must have 6 neighbours (less than that means it's a border SSU)\n    # - Must have at least 1 - `prop_na` of non empty pixels\n    # - 4 out 6 neighbours must also respect the above rule\n    SSU_selected = SSUs |&gt;\n        group_by(ET_Index) |&gt;\n        mutate(\n            nbNeighb = map_dbl(\n                ssuID,\n                ~ sum(neighbour_ls[[unique(ET_Index)]][, .x]) - 1\n            ),\n            nbPropNA = map_int(\n                ssuID,\n                .f = function(x, pNA, ETI)\n                    sum(\n                        pNA[\n                            setdiff(which(neighbour_ls[[ETI]][, x]), x)\n                        ] &lt;= prop_na\n                    ),\n                pNA = propNA,\n                ETI = unique(ET_Index)\n            ),\n            sampled = sample_SSU(\n                ssuid = ssuID,\n                prob = incl_prob,\n                geom = geometry,\n                filtered = propNA &lt;= prop_na & nbNeighb == 6  & nbPropNA &gt;= 4,\n                ssuDist = ssu_dist,\n                N = ssu_N\n            )\n        ) |&gt;\n        ungroup()\n\n    # Prepare selected SSU and their specific neighbours with code like `A_B`\n    # `A` is for the SSU sample ID (1:`ssu_N`)\n    # `B` is for the neighbour ID (0:6 where zero is the focal point, and\n    # 1:6 are the 6 neighbours starting from the top point moving clockwise)\n    SSU_lyr &lt;- subset(SSU_selected, sampled &gt; 0)\n    SSU_lyr_ls &lt;- list()\n\n    for(i in 1:nrow(SSU_lyr))\n    {\n        # get neighbours for specific row\n        nei_hx &lt;- SSU_selected |&gt; \n            filter(ET_Index == SSU_lyr$ET_Index[i]) |&gt; \n            filter(\n                ssuID %in% which(\n                    neighbour_ls[[SSU_lyr$ET_Index[i]]][, SSU_lyr$ssuID[i]]\n                )\n            )\n\n        # code A \n        nei_hx$codeA &lt;- SSU_lyr$sampled[i]\n        \n        # code B\n        nei_hx$codeB &lt;- c(4, 3, 5, 0, 2, 6, 1)\n\n        SSU_lyr_ls[[i]] &lt;- nei_hx\n    }\n\n    # save\n    assign(\n        paste0('SSU_all_', lyr),\n        SSU_selected\n    )\n    assign(\n        paste0('SSU_', lyr),\n        do.call(rbind, SSU_lyr_ls)\n    )\n}"
  },
  {
    "objectID": "runGRTSexample.html#export-psu-and-ssu-samples-in-shapefiles",
    "href": "runGRTSexample.html#export-psu-and-ssu-samples-in-shapefiles",
    "title": "8  A guide for BMS sampling design with R",
    "section": "Export PSU and SSU samples in shapefiles",
    "text": "Export PSU and SSU samples in shapefiles\nThe purpose of this final section is to export all PSU and SSU points in a shapefile format. The format was chosen to facilitate post-processing, but it can be modified to fit different needs. The first code chunk saves all PSU and SSU points into a single file, while the second code chunk separates the same sampled points by ecoregion.\n\nGrouped ecoregions\n\n\nCode\nsavePath = file.path(outputFolder, 'allEcoregion')\ndir.create(savePath, recursive = TRUE)\n\nvarsToRm = c(\n    'OBJECTID', 'Join_Count', 'TARGET_FID',\n    'ET_ID', 'ET_ID_Old', 'ET_IDX_Old'\n)\n\nhexas_save &lt;- hexas |&gt;\n    select(-all_of(varsToRm))\n\n# rename attributes table so it has a maximum of 10 characters\nnames(hexas_save) &lt;- abbreviate(names(hexas_save), minlength = 10)\n\n# add coordinates\ncoords &lt;- hexas_save |&gt;\n    sf::st_centroid() |&gt;\n    sf::st_transform(4326) |&gt;\n    sf::st_coordinates() |&gt;\n    as.data.frame()\n\nhexas_save &lt;- hexas_save |&gt;\n    mutate(\n        latitude = coords$Y,\n        longitude = coords$X\n    )\n\n# save all PSU\nhexas_save |&gt;\n    write_sf(\n        file.path(\n            savePath,\n            paste0('PSU-SOBQ_ALL-', fileSuffix, '.shp')\n        )\n    )\n\n# Selected PSUs\nfor(lyr in c('main', 'over', 'extra'))\n{\n    hexas_save |&gt;\n        filter(\n            ET_Index %in% get(paste0('selected_', lyr))$ET_Index\n        ) |&gt;\n        write_sf(\n            file.path(\n                savePath,\n                paste0('PSU-SOBQ_', lyr, '-', fileSuffix, '.shp')\n            )\n        )\n}\n\n# Save all SSU\nfor(lyr in c('main', 'over', 'extra'))\n{\n    # SSU main\n    SSU_lyr &lt;- get(paste0('SSU_all_', lyr)) |&gt;\n        select(-c('nbNeighb', 'nbPropNA', 'sampled'))\n\n    coords &lt;- SSU_lyr |&gt;\n        sf::st_transform(4326) |&gt;\n        sf::st_coordinates() |&gt;\n        as.data.frame()\n\n    SSU_lyr |&gt;\n        mutate(\n            latitude = coords$Y,\n            longitude = coords$X\n        ) |&gt;\n        write_sf(\n            file.path(\n                savePath,\n                paste0('SSU-SOBQ_ALL_', lyr, '-', fileSuffix, '.shp')\n            )\n        )\n}\n\n# Save selected SSU\nfor(lyr in c('main', 'over', 'extra'))\n{\n    # SSU main\n    SSU_lyr &lt;- get(paste0('SSU_', lyr)) |&gt;\n        select(-c('nbNeighb', 'nbPropNA', 'sampled'))\n\n    coords &lt;- SSU_lyr |&gt;\n        sf::st_transform(4326) |&gt;\n        sf::st_coordinates() |&gt;\n        as.data.frame()\n\n    SSU_lyr |&gt;\n        mutate(\n            latitude = coords$Y,\n            longitude = coords$X\n        ) |&gt;\n        write_sf(\n            file.path(\n                savePath,\n                paste0('SSU-SOBQ_selected_', lyr, '-', fileSuffix, '.shp')\n            )\n        )\n}\n\n\n\n\nSplited ecoregions\n\n\nCode\nfor(eco in eco_sim)\n{\n    # create folder\n    eco_path &lt;- file.path(\n        outputFolder,\n        'byEcoregion',\n        paste0('ecoregion_', eco)\n    )\n    dir.create(eco_path, recursive = TRUE)\n\n    # PSU\n    ###########################################\n    hexas_save |&gt;\n        filter(ecoregion == eco) |&gt;\n        write_sf(\n            file.path(\n                eco_path,\n                paste0('PSU-SOBQ_eco', eco, '_ALL-', fileSuffix, '.shp')\n            )\n        )\n\n    for(lyr in c('main', 'over', 'extra'))\n    {\n        hexas_save |&gt;\n            filter(\n                ET_Index %in% subset(\n                    get(paste0('selected_', lyr)),\n                    ecoregion == eco\n                )$ET_Index\n            ) |&gt;\n            write_sf(\n                file.path(\n                    eco_path,\n                    paste0(\n                        'PSU-SOBQ_eco',\n                        eco, '_',\n                        lyr, '-',\n                        fileSuffix,\n                        '.shp'\n                    )\n                )\n            )\n    }\n\n    # SSU\n    ###########################################\n\n    # Save all SSU\n    for(lyr in c('main', 'over', 'extra'))\n    {\n        # SSU main\n        SSU_lyr &lt;- get(paste0('SSU_all_', lyr)) |&gt;\n            filter(ecoregion == eco) |&gt;\n            select(-c('nbNeighb', 'nbPropNA', 'sampled'))\n\n        coords &lt;- SSU_lyr |&gt;\n            sf::st_transform(4326) |&gt;\n            sf::st_coordinates() |&gt;\n            as.data.frame()\n\n        SSU_lyr |&gt;\n            mutate(\n                latitude = coords$Y,\n                longitude = coords$X\n            ) |&gt;\n            write_sf(\n                file.path(\n                    eco_path,\n                    paste0(\n                        'SSU-SOBQ_eco',\n                        eco,\n                        '_ALL_',\n                        lyr, '-',\n                        fileSuffix,\n                        '.shp'\n                    )\n                )\n            )\n    }\n\n    # Save selected SSU\n    for(lyr in c('main', 'over', 'extra'))\n    {\n        # SSU main\n        SSU_lyr &lt;- get(paste0('SSU_', lyr)) |&gt;\n            filter(ecoregion == eco) |&gt;\n            select(-c('nbNeighb', 'nbPropNA', 'sampled'))\n\n        coords &lt;- SSU_lyr |&gt;\n            sf::st_transform(4326) |&gt;\n            sf::st_coordinates() |&gt;\n            as.data.frame()\n\n        SSU_lyr |&gt;\n            mutate(\n                latitude = coords$Y,\n                longitude = coords$X\n            ) |&gt;\n            write_sf(\n                file.path(\n                    eco_path,\n                    paste0(\n                        'SSU-SOBQ_eco',\n                        eco,\n                        '_selected_',\n                        lyr, '-',\n                        fileSuffix,\n                        '.shp'\n                    )\n                )\n            )\n    }\n}"
  },
  {
    "objectID": "ecosumm_habitat.html#ecoregion-7",
    "href": "ecosumm_habitat.html#ecoregion-7",
    "title": "9  Ecoregion summary: habitat",
    "section": "Ecoregion 7",
    "text": "Ecoregion 7\n## Ecoregion 28\n## Ecoregion 30\n## Ecoregion 31\n## Ecoregion 46\n## Ecoregion 47\n## Ecoregion 48\n## Ecoregion 49\n## Ecoregion 72\n## Ecoregion 73\n## Ecoregion 74\n## Ecoregion 75\n## Ecoregion 76\n## Ecoregion 77\n## Ecoregion 78\n## Ecoregion 86\n## Ecoregion 96\n## Ecoregion 99\n## Ecoregion 100\n## Ecoregion 101\n## Ecoregion 102\n## Ecoregion 103\n## Ecoregion 117\n## Ecoregion 216\n## Ecoregion 217"
  },
  {
    "objectID": "ecosumm_cost.html#ecoregion-7",
    "href": "ecosumm_cost.html#ecoregion-7",
    "title": "10  Ecoregion summary: cost",
    "section": "Ecoregion 7",
    "text": "Ecoregion 7\n## Ecoregion 28\n## Ecoregion 30\n## Ecoregion 31\n## Ecoregion 46\n## Ecoregion 47\n## Ecoregion 48\n## Ecoregion 49\n## Ecoregion 72\n## Ecoregion 73\n## Ecoregion 74\n## Ecoregion 75\n## Ecoregion 76\n## Ecoregion 77\n## Ecoregion 78\n## Ecoregion 86\n## Ecoregion 96\n## Ecoregion 99\n## Ecoregion 100\n## Ecoregion 101\n## Ecoregion 102\n## Ecoregion 103\n## Ecoregion 117\n## Ecoregion 216\n## Ecoregion 217"
  },
  {
    "objectID": "ecosumm_legacySites.html#ecoregion-100",
    "href": "ecosumm_legacySites.html#ecoregion-100",
    "title": "11  Ecoregion summary: historical sites",
    "section": "Ecoregion 100",
    "text": "Ecoregion 100\n## Ecoregion 101\n## Ecoregion 102\n## Ecoregion 103\n## Ecoregion 117\n## Ecoregion 217\n## Ecoregion 47\n## Ecoregion 72\n## Ecoregion 74\n## Ecoregion 75\n## Ecoregion 76\n## Ecoregion 96\n## Ecoregion 99"
  },
  {
    "objectID": "ecosumm_selection.html#ecoregion-7",
    "href": "ecosumm_selection.html#ecoregion-7",
    "title": "12  Ecoregion summary: selected hexagons",
    "section": "Ecoregion 7",
    "text": "Ecoregion 7\n## Ecoregion 28\n## Ecoregion 30\n## Ecoregion 31\n## Ecoregion 46\n## Ecoregion 47\n## Ecoregion 48\n## Ecoregion 49\n## Ecoregion 72\n## Ecoregion 73\n## Ecoregion 74\n## Ecoregion 75\n## Ecoregion 76\n## Ecoregion 77\n## Ecoregion 78\n## Ecoregion 86\n## Ecoregion 96\n## Ecoregion 99\n## Ecoregion 100\n## Ecoregion 101\n## Ecoregion 103\n## Ecoregion 216\n## Ecoregion 217"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Foster, Scott D. 2021. “MBHdesign: An r-Package for Efficient\nSpatial Survey Designs.” Methods in Ecology and\nEvolution 12 (3): 415–20. https://doi.org/https://doi.org/10.1111/2041-210X.13535.\n\n\nLatifovic, Rasim, Colin Homer, Rainer Ressl, Darren Pouliot, Sheikh\nNazmul Hossain, René R Colditz, Ian Olthof, Chandra P Giri, and Arturo\nVictoria. 2016. “20 North American Land-Change Monitoring\nSystem.” Remote Sensing of Land Use and Land Cover, 303.\n\n\nStevens Jr, Don L, and Anthony R Olsen. 2004. “Spatially Balanced\nSampling of Natural Resources.” Journal of the American\nStatistical Association 99 (465): 262–78.\n\n\nVan Wilgenburg, C. Lisa AND Campbell, Steven L. AND Mahon. 2020.\n“A Cost Efficient Spatially Balanced Hierarchical Sampling Design\nfor Monitoring Boreal Birds Incorporating Access Costs and Habitat\nStratification.” PLOS ONE 15 (6): 1–28. https://doi.org/10.1371/journal.pone.0234494."
  }
]